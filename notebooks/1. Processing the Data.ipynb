{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import multiprocess as mp\n",
    "\n",
    "from typing import Dict, List, Any, Tuple, Callable, Union\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in the data\n",
    "## 1.1 Load and Join Labels\n",
    "This code loads in my bank transactions which consists of a date, amount, description and account balance. Then, if the transaction appears in my labelled dataset, it adds its labelled class with a join. \n",
    "\n",
    "A transaction is considered the same if it has the same description and amount to account for the fact that some purchases with the same description are in different classes (e.g. fuel vs. food is differentiated by the amount spent at the petrol station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13d4c7430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set path\n",
    "path = \"../components/private_files/\"\n",
    "\n",
    "# Load in data\n",
    "\n",
    "data = pd.read_csv(path + \"CSVData.csv\", header=None)\n",
    "data.columns = [\"date\", \"amount\", \"description\", \"balance\"]\n",
    "data['date']  = pd.to_datetime(data['date'], format='%d/%m/%Y')\n",
    "data = data.drop(\"balance\", axis=1)\n",
    "data = data.astype({'amount':'float'})\n",
    "\n",
    "# Load in labels and join to data\n",
    "\n",
    "labels = pd.read_csv(path + \"transactions_labelled.csv\")\n",
    "labels = labels.drop(\"date\", axis=1)\n",
    "labels = labels.drop_duplicates()\n",
    "\n",
    "\n",
    "labels.description = labels.description.str.strip()\n",
    "labels.description = labels.description.str.lower()\n",
    "data.description = data.description.str.strip()\n",
    "data.description = data.description.str.lower()\n",
    "\n",
    "\n",
    "data_labs = data.merge(labels, on=[\"description\", \"amount\"], how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "data_labs.category.value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x13d5a5430>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge some of the underrepresented labels\n",
    "\n",
    "data_labs.category = [\"beers\" if cat == \"entertainment\" else cat for cat in data_labs.category]\n",
    "data_labs.category = [\"beers\" if cat == \"booze\" else cat for cat in data_labs.category]\n",
    "data_labs.category = [\"wages\" if cat == \"tutoring\" else cat for cat in data_labs.category]\n",
    "data_labs.category = [\"health\" if cat == \"education\" else cat for cat in data_labs.category]\n",
    "data_labs.category = [\"life/wellbeing\" if cat == \"health\" else cat for cat in data_labs.category]\n",
    "data_labs.category = [\"transfer\" if cat == \"donation\" else cat for cat in data_labs.category]\n",
    "data_labs.category = [\"transport\" if cat in [\"uber\", \"public transport\", \"fuel\"] else cat for cat in data_labs.category]\n",
    "\n",
    "data_labs.reset_index()\n",
    "data_labs.category.value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This narrows down the categories to 7:\n",
    "1. food\n",
    "2. transfers\n",
    "3. beers\n",
    "4. transport\n",
    "5. wages\n",
    "6. shopping\n",
    "7. life/wellbeing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Scrape \"Value Date\" from Descriptions\n",
    "Some transactions are not processed immediately at the point of sale. When the bank transaction is finally approved, my bank records this as the date and appends the actual date of the transaction (which may have been up to 5 days in the past, the average I found was a 3 day delay) to the end of the description as \"Value Date: dd/mm/YYYY\". If the transaction goes through immediately, it just records the current date and leaves the description as is. (At least, this is the process that I inferred. I was unable to find any documentation to confirm this.)\n",
    "\n",
    "Hence, I had to do a little scraping to collect these value dates when they appeared so that I had the date of the actual transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>amount</th>\n",
       "      <th>category</th>\n",
       "      <th>value_date</th>\n",
       "      <th>trans_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>50.0</td>\n",
       "      <td>wages</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>food</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-07-26</td>\n",
       "      <td>50.0</td>\n",
       "      <td>wages</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-07-25</td>\n",
       "      <td>-14.5</td>\n",
       "      <td>food</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2020-07-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>2019-09-13</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>beers</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>2019-09-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>-42.2</td>\n",
       "      <td>food</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>2019-09-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>beers</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>2019-09-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>2019-09-12</td>\n",
       "      <td>19.0</td>\n",
       "      <td>food</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>2019-09-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>food</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2019-09-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  amount category value_date trans_date\n",
       "0   2020-07-27    50.0    wages        NaT 2020-07-27\n",
       "1   2020-07-27    -4.8     food        NaT 2020-07-27\n",
       "2   2020-07-27    -4.0     food        NaT 2020-07-27\n",
       "3   2020-07-26    50.0    wages        NaT 2020-07-26\n",
       "4   2020-07-25   -14.5     food        NaT 2020-07-25\n",
       "..         ...     ...      ...        ...        ...\n",
       "595 2019-09-13   -36.0    beers 2019-09-11 2019-09-11\n",
       "596 2019-09-12   -42.2     food 2019-09-09 2019-09-09\n",
       "597 2019-09-12   -13.0    beers 2019-09-06 2019-09-06\n",
       "598 2019-09-12    19.0     food 2019-09-09 2019-09-09\n",
       "599 2019-09-11    -5.0     food        NaT 2019-09-11\n",
       "\n",
       "[600 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape descriptions for \"value date\" (actual date of transaction)\n",
    "\n",
    "value_dates = []\n",
    "for desc in data_labs.description:\n",
    "    if \"value date: \" in desc:\n",
    "        val_date = desc.split(\"value date: \")[1]\n",
    "        val_date = val_date.strip()\n",
    "        value_dates.append(val_date)\n",
    "    else:\n",
    "        value_dates.append(None)\n",
    "value_dates = np.array(value_dates)\n",
    "data_labs[\"value_date\"] = value_dates\n",
    "data_labs.value_date = pd.to_datetime(data_labs.value_date, format='%d/%m/%Y')\n",
    "\n",
    "# If there is no value date in the description, \n",
    "# it is assumed that the date of the record is the date of the transaction.\n",
    "\n",
    "transaction_date = []\n",
    "for index, row in data_labs.iterrows():\n",
    "    if pd.isnull(row.value_date):\n",
    "        transaction_date.append(row.date)\n",
    "    else:\n",
    "        transaction_date.append(row.value_date)\n",
    "        \n",
    "data_labs[\"trans_date\"] = transaction_date\n",
    "data_labs.drop(\"description\", axis=1) # Not showing description for privacy reasons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction\n",
    "\n",
    "## 2.1 Weekday\n",
    "Next, I wanted to pull out some interesting features. I figured whether the transaction occurred on a weekend would be telling. Initially I was going to just use a boolean isWeekend column marking when the weekday was 4, 5 or 6 (friday, saturday or sunday), but then I thought to let the models determine a cut off naturally, leaving the weekday as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>wages</td>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.8</td>\n",
       "      <td>food</td>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>food</td>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>wages</td>\n",
       "      <td>2020-07-26</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.5</td>\n",
       "      <td>food</td>\n",
       "      <td>2020-07-25</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>-36.0</td>\n",
       "      <td>beers</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>-42.2</td>\n",
       "      <td>food</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>beers</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>19.0</td>\n",
       "      <td>food</td>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>food</td>\n",
       "      <td>2019-09-11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     amount category       date  weekday\n",
       "0      50.0    wages 2020-07-27        0\n",
       "1      -4.8     food 2020-07-27        0\n",
       "2      -4.0     food 2020-07-27        0\n",
       "3      50.0    wages 2020-07-26        6\n",
       "4     -14.5     food 2020-07-25        5\n",
       "..      ...      ...        ...      ...\n",
       "595   -36.0    beers 2019-09-11        2\n",
       "596   -42.2     food 2019-09-09        0\n",
       "597   -13.0    beers 2019-09-06        4\n",
       "598    19.0     food 2019-09-09        0\n",
       "599    -5.0     food 2019-09-11        2\n",
       "\n",
       "[600 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data = data_labs.drop([\"date\", \"value_date\"], axis=1)\n",
    "tr_data.columns = [\"amount\", \"description\", \"category\", \"date\"]\n",
    "# tr_data[\"weekend\"] = [True if day >= 4 else False for day in tr_data.weekday]\n",
    "# tr_data = tr_data.drop(\"weekday\", axis=1)\n",
    "tr_data[\"weekday\"] = tr_data.date.dt.weekday\n",
    "tr_data.drop(\"description\", axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Natural Language Processing\n",
    "\n",
    "### 2.2.0 Overview\n",
    "\n",
    "Bank descriptions are limited to only a few characters. Hence, the text differs to most NLP texts in a few areas:\n",
    "* Most of the words are meaningful\n",
    "* Shortened words are common\n",
    "\n",
    "The general approach I have decided to take to extract features from the descriptions of each transaction is as follows:\n",
    "1. Clean the descriptions (remove numbers, punctuation, single-letter words, stopwords, repeated words etc.)\n",
    "2. On the training data, collect a corpus of words for each class. This is a list of words that is loosely indicative of the class itself. A naive approach would be to add all of the unique words that appear in the cleaned descriptions of a given class to that class's corpus. We also add \"chopped\" versions of each word which will be discussed later.\n",
    "3. For each entry in the test set, measure how closely its cleaned description resembles the corpus of each class. Create a column for each class that quantitatively represents how close that entry's description is to the corpus of each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Cleaning The Descriptions\n",
    "\n",
    "#### Blacklist\n",
    "Due to the first dot point above, stopwords are not that common. But in the rare case they appear, it wouldn't hurt to filter them out so that's what we do. I also add a few terms to the blacklist that appear commonly at the end of most descriptions and are hence meaningless in terms of class differentiation, e.g. \"card\", \"aus\", \"ns\", etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = stopwords.words('english')\n",
    "blacklist += ['card', 'aus', 'au', 'ns', 'nsw', 'xx', 'pty', 'ltd', 'nswau']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "Now that we have defined a blacklist, we can build our cleaning function. Firstly, note that the descriptions have already been converted to lowercase. With this in mind, we go on to remove the \"value date\" aspect of the description if present. Then we remove all non-alphanumeric characters, replacing them with a space. We then remove all words that are either in the blacklist, are shorter than two letters or that are duplicates of a word that has already passed the checks previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean(string:str, blacklist:List[str]) -> str:\n",
    "    features = \"\"\n",
    "    out = string\n",
    "    if \"value date: \" in out:\n",
    "        out = out.split(\"value date: \")[0]\n",
    "    out = re.sub(r'([^a-z ]+)', \" \", out) # remove non-alphabetic characters\n",
    "    for word in out.strip().split(\" \"):\n",
    "        if word not in blacklist and len(word) > 1 and \" \" + word + \" \" not in features: # ensuring no duplicates\n",
    "            features += word + \" \"\n",
    "    return features.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the above function to create a column for the cleaned descriptions, called \"desc_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data[\"desc_features\"] = [clean(desc, blacklist) for desc in tr_data.description]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Collecting Each Class Corpus\n",
    "\n",
    "#### N-Grams (aka \"The Chop\")\n",
    "To address the fact that shortened words are common, I proposed a very naive approach were I essentially \"chop up\" larger words down to a length of three in the following manner:\n",
    "If given the word \"McDonalds\", we would add to our corpus the words \"McDonalds\", \"McDonald\", \"McDonal\", \"McDona\", \"McDon\", \"McDo\", \"McD\". \n",
    "\n",
    "Essentially, this is adding all character n-grams but rooting the starting character.\n",
    "\n",
    "A convincing reason why this isn't a horrible idea is that my bank appends the \"Value Date\" to the end of the description, overwriting the description if it becomes too long. This results in words being cut off frequently. \n",
    "\n",
    "\n",
    "#### Extracting the Candidates to Build Each Class Corpus\n",
    "By adding the chopped words along with the words themselves as candidates for each class corpus, we make each corpus invariant to the \"value date cutoff\" phenomenon. \n",
    "\n",
    "We implement this technique in the following function. Similar to the *clean* function above, but adding in the rooted character n-grams, where n>2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_chop(string:str, blacklist:List[str]) -> str:\n",
    "    corpus = \"\"\n",
    "    out = string\n",
    "    if \"value date: \" in out:\n",
    "        out = out.split(\"value date: \")[0]\n",
    "    out = re.sub(r'([^a-z ]+)', \" \", out) # remove non-alphabetic characters\n",
    "    for word in out.strip().split(\" \"):\n",
    "        if word not in blacklist and len(word) > 1 and \" \" + word + \" \" not in corpus: # ensuring no duplicates\n",
    "            wrd = word\n",
    "            while len(wrd) > 2:\n",
    "                corpus += wrd + \" \"\n",
    "                wrd = wrd[0:-1]\n",
    "    return corpus.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vowel Removal\n",
    "A limitation to the approach above is that we in no way account for deliberate word shortening i.e. removal of vowels (\"rmvl of vwls\"). I did try to implement a de-vowel method to maybe account for this, but it actually resulted in diminished performance so I scrapped the idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_chop_devowel(string):\n",
    "#     corpus = \"\"\n",
    "#     out = string\n",
    "#     if \"value date: \" in out:\n",
    "#         out = out.split(\"value date: \")[0]\n",
    "#     out = re.sub(r'([^a-z ]+)', \" \", out) # remove non-alphabetic characters\n",
    "#     out = out.strip().split(\" \")\n",
    "#     for word in out:\n",
    "#         if word not in blacklist and len(word) > 1 and \" \" + word + \" \" not in corpus: # ensuring no duplicates\n",
    "#             corpus += re.sub(r'([aeiou])+', \"\", word) + \" \" # Add devowelled word\n",
    "#             wrd = word\n",
    "#             while len(wrd) >= 3:\n",
    "#                 corpus += wrd + \" \"\n",
    "#                 wrd = wrd[0:-1]\n",
    "#     return corpus.strip()\n",
    "\n",
    "# def clean_devowel(string):\n",
    "#     features = \"\"\n",
    "#     out = string\n",
    "#     if \"value date: \" in out:\n",
    "#         out = out.split(\"value date: \")[0]\n",
    "#     out = re.sub(r'([^a-z ]+)', \" \", out) # remove non-alphabetic characters\n",
    "#     for word in out.strip().split(\" \"):\n",
    "#         if word not in blacklist and len(word) > 1 and \" \" + word + \" \" not in features: # ensuring no duplicates\n",
    "#             features += re.sub(r'([aeiou])+', \"\", word) + \" \"\n",
    "#             features += word + \" \"\n",
    "#     return features.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting Candidates\n",
    "\n",
    "Here, we implement the function defined above to extract the candidates which we will build our class corpora on. Below, you can see the output for a specific entry involving McDonalds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>weekday</th>\n",
       "      <th>desc_features</th>\n",
       "      <th>desc_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.8</td>\n",
       "      <td>mcdonalds gladesville gladesville  nswau</td>\n",
       "      <td>food</td>\n",
       "      <td>2020-07-27</td>\n",
       "      <td>0</td>\n",
       "      <td>mcdonalds gladesville</td>\n",
       "      <td>mcdonalds mcdonald mcdonal mcdona mcdon mcdo m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount                               description category       date  \\\n",
       "1    -4.8  mcdonalds gladesville gladesville  nswau     food 2020-07-27   \n",
       "\n",
       "   weekday          desc_features  \\\n",
       "1        0  mcdonalds gladesville   \n",
       "\n",
       "                                         desc_corpus  \n",
       "1  mcdonalds mcdonald mcdonal mcdona mcdon mcdo m...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data[\"desc_corpus\"] = [clean_chop(desc, blacklist) for desc in tr_data.description]\n",
    "tr_data.iloc[[1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Each Class Corpus\n",
    "Here, I define a get_corpus function that will collect the corpus of each class. The idea is to collect a list of words for each class. The list of words will in some way characterise the class. I added a threshold parameter which I can adjust to only count words that appear a certain number of times in a particular class. If the word \"panther\" appears in only one \"food\" entry, then its unlikely that a description containing \"panther\" would generalise to every food item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpora(training:pd.DataFrame, min_freq=0) -> Dict[str, List[str]]:\n",
    "    try:\n",
    "        assert set([\"category\", \"desc_corpus\"]).issubset(set(training.columns))\n",
    "    except:\n",
    "        raise InvalidDataFrameFormat(training, message=\"'category' and/or 'desc_corpus' columns not found. Could not extract corpora from dataframe.\")\n",
    "\n",
    "    global_corpora:Dict[str, List[str]] = {}\n",
    "    for cat in training.category.unique():\n",
    "        corpus:Dict[str, int] = {}\n",
    "        for index, row in training[training.category == cat].iterrows():    # Counting appearances of each corpus candidate within the class\n",
    "            words = row.desc_corpus.split(\" \")\n",
    "            for word in words:\n",
    "                if word not in corpus.keys():\n",
    "                    corpus[word] = 1\n",
    "                else:\n",
    "                    corpus[word] += 1\n",
    "        \n",
    "        # Order the words in descending frequency\n",
    "        words = {k: v for k, v in sorted(corpus.items(), key=lambda item: (item[1], 1/len(item[0])), reverse=True)}\n",
    "        freq = np.array(list(words.values()))\n",
    "        perc = freq/training[training.category == cat].shape[0]\n",
    "        features = []\n",
    "        for word, count in zip(words.keys(), freq):\n",
    "            if count > min_freq:\n",
    "                features.append(word)\n",
    "        global_corpora[cat] = features\n",
    "    return global_corpora\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Quantitatively Determine the \"Likeness\" of Each Description to Each Class Corpus\n",
    "\n",
    "Now, for each entry, we compare its description features (the description stripped of all numbers and punctuation) to the corpus of each class (found with the function above). Firstly, I take each word in the entry's `desc_features` and compare its similarity to each word in the corpus of a given class (using the jaccard distance because its computationally efficient). It then tries all words in the class corpus and returns the minimum (this will be 0 if it finds an exact match). It does this for all the words in the description features, then takes an average of the minimum distances. This is how I estimate the \"closeness\" of each description to the class corpus of each class.\n",
    "\n",
    "We then store each entry's \"closeness\" to each class in separate columns, one column per class labelled `<class>_desc_dist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance = nltk.edit_distance\n",
    "def distance(string1:str, string2:str) -> float:\n",
    "    return nltk.jaccard_distance(set(string1), set(string2))\n",
    "\n",
    "def desc_dist(corpus:List[str], desc:str, distance:Callable=distance) -> float:\n",
    "    if len(corpus) == 0:\n",
    "        return 100.0\n",
    "    min_distances = []\n",
    "    for feature in desc.split(\" \"):\n",
    "        min_dist = np.inf\n",
    "        for corp in corpus:                 # Find the minimum distance that each feature has to any corpus item (best possible match)\n",
    "            dist = distance(feature, corp)\n",
    "            if dist == 0.0:\n",
    "                min_dist = dist\n",
    "                break\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "        min_distances.append(min_dist)\n",
    "    return np.mean(min_distances)           # Return the mean \"best case\" distance \n",
    "\n",
    "\n",
    "def NLP_distances(dat:pd.DataFrame, corpus:Dict[str, List[str]], desc_dist:Callable=desc_dist) -> pd.DataFrame:\n",
    "    data=dat.copy()\n",
    "    try:\n",
    "        assert set([\"description\", \"date\", \"desc_features\", \"desc_corpus\"]).issubset(set(data.columns))\n",
    "    except:\n",
    "        raise InvalidDataFrameFormat(data, message='Missing some column(s) from the set: {\"description\", \"date\", \"desc_features\", \"desc_corpus\"}')\n",
    "\n",
    "    for cat, corp in corpus.items():\n",
    "        data[cat + \"_desc_dist\"] = [desc_dist(corp, desc) for desc in data.desc_features]\n",
    "    data = data.drop([\"description\", \"date\", \"desc_features\", \"desc_corpus\"], axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above functions, we have the full set of features to train our model with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>category</th>\n",
       "      <th>weekday</th>\n",
       "      <th>wages_desc_dist</th>\n",
       "      <th>food_desc_dist</th>\n",
       "      <th>transfer_desc_dist</th>\n",
       "      <th>life/wellbeing_desc_dist</th>\n",
       "      <th>shopping_desc_dist</th>\n",
       "      <th>transport_desc_dist</th>\n",
       "      <th>beers_desc_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.0</td>\n",
       "      <td>wages</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427579</td>\n",
       "      <td>0.265741</td>\n",
       "      <td>0.363889</td>\n",
       "      <td>0.451190</td>\n",
       "      <td>0.497024</td>\n",
       "      <td>0.396296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.8</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "      <td>0.449495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "      <td>0.449495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.0</td>\n",
       "      <td>wages</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.520909</td>\n",
       "      <td>0.492381</td>\n",
       "      <td>0.443810</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-14.5</td>\n",
       "      <td>food</td>\n",
       "      <td>5</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>-36.0</td>\n",
       "      <td>beers</td>\n",
       "      <td>2</td>\n",
       "      <td>0.352273</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>-42.2</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>-13.0</td>\n",
       "      <td>beers</td>\n",
       "      <td>4</td>\n",
       "      <td>0.442857</td>\n",
       "      <td>0.198571</td>\n",
       "      <td>0.355238</td>\n",
       "      <td>0.465714</td>\n",
       "      <td>0.491429</td>\n",
       "      <td>0.313333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>19.0</td>\n",
       "      <td>food</td>\n",
       "      <td>0</td>\n",
       "      <td>0.369643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>food</td>\n",
       "      <td>2</td>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     amount category  weekday  wages_desc_dist  food_desc_dist  \\\n",
       "0      50.0    wages        0         0.000000        0.427579   \n",
       "1      -4.8     food        0         0.449495        0.000000   \n",
       "2      -4.0     food        0         0.449495        0.000000   \n",
       "3      50.0    wages        6         0.000000        0.339286   \n",
       "4     -14.5     food        5         0.357143        0.000000   \n",
       "..      ...      ...      ...              ...             ...   \n",
       "595   -36.0    beers        2         0.352273        0.166667   \n",
       "596   -42.2     food        0         0.350000        0.000000   \n",
       "597   -13.0    beers        4         0.442857        0.198571   \n",
       "598    19.0     food        0         0.369643        0.000000   \n",
       "599    -5.0     food        2         0.522727        0.000000   \n",
       "\n",
       "     transfer_desc_dist  life/wellbeing_desc_dist  shopping_desc_dist  \\\n",
       "0              0.265741                  0.363889            0.451190   \n",
       "1              0.422222                  0.187500            0.187500   \n",
       "2              0.422222                  0.187500            0.187500   \n",
       "3              0.400000                  0.520909            0.492381   \n",
       "4              0.342857                  0.142857            0.250000   \n",
       "..                  ...                       ...                 ...   \n",
       "595            0.500000                  0.400000            0.222222   \n",
       "596            0.266667                  0.200000            0.266667   \n",
       "597            0.355238                  0.465714            0.491429   \n",
       "598            0.200000                  0.150000            0.200000   \n",
       "599            0.500000                  0.083333            0.083333   \n",
       "\n",
       "     transport_desc_dist  beers_desc_dist  \n",
       "0               0.497024         0.396296  \n",
       "1               0.187500         0.187500  \n",
       "2               0.187500         0.187500  \n",
       "3               0.443810         0.375000  \n",
       "4               0.272727         0.222222  \n",
       "..                   ...              ...  \n",
       "595             0.500000         0.000000  \n",
       "596             0.150000         0.111111  \n",
       "597             0.313333         0.000000  \n",
       "598             0.154167         0.166667  \n",
       "599             0.083333         0.083333  \n",
       "\n",
       "[600 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data = tr_data[~pd.isnull(tr_data.category)]\n",
    "\n",
    "labelled_data = NLP_distances(labelled_data, get_corpora(labelled_data))\n",
    "\n",
    "labelled_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scaling\n",
    "Some of the models I want to use (e.g. nearest neighbour) will require the data be scaled. Here, I create a function that will take in our scaler (which we will fit on the training data) and scale our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(data:pd.DataFrame, scaler:StandardScaler) -> pd.DataFrame:\n",
    "    norm = scaler.transform(data)\n",
    "    norm_data = pd.DataFrame(norm, columns = data.columns, index = data.index)\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementing a \"Lookup\" functionality\n",
    "\n",
    "If an entry of the test data has the exact same description as some example in the training data, then we can assume it has the same class as the training example.\n",
    "\n",
    "Firstly, we ensure that we discount any training examples which have the same description but have multiple classes (such as fuel and food for petrol stations). Then we can safely merge the test entries onto the training examples using `desc_features` as the key. The following function implements this, returning an array of predicted classes if found, otherwise NaN if the class wasn't found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookup(X_train, X_test):\n",
    "    table = pd.DataFrame(X_train[[\"desc_features\", \"category\"]].groupby([\"desc_features\"])[\"category\"].unique().apply(','.join))\n",
    "    table = table.reset_index()\n",
    "    table = table[~table.category.str.contains(\",\")]\n",
    "    lookup_predictions = np.array(X_test.drop(\"category\", axis=1).merge(table, on=\"desc_features\", how=\"left\", validate=\"many_to_one\").category, dtype=object)\n",
    "    return lookup_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implementing a \"Webscrape\" functionality\n",
    "\n",
    "## 5.1 Scraping Results\n",
    "The next idea I had to improve the accuracy of my model is to abuse Google's suggestions sidebar. When you google something like a restaurant, most of the time Google will give you a side-bar showcasing the business that it thinks best suits your search query. The side-bar has an html element which gives the business a category. For example, when I google \"McDonald Gladesville\" (the least creative example I could think of), Google tells me that this is a fast food restaurant. \n",
    "\n",
    "<img src=\"files/google_sidebar_example.png\">\n",
    "\n",
    "Thankfully, it turns out that the class of this HTML element is always labelled \"YhemCb\", meaning we can easily scrape it with BeautifulSoup, retrieving the HTML element:\n",
    "\n",
    "`<span class=\"YhemCb\"><span>Fast food restaurant</span></span>`\n",
    "\n",
    "My experimentation also found that if you google a larger company, similar details are stored in a class labelled \"wwUB2c PZPZlf\" i.e. when googling \"McDonalds\" we would want to retrieve this HTML element to get that it is a \"Fast food company\":\n",
    "\n",
    "`<div class=\"wwUB2c PZPZlf\" data-attrid=\"subtitle\"><span data-ved=\"2ahUKEwiZsZLXspLrAhXYeisKHTRvDdoQ2kooAjAcegQIGhAO\">Fast food company</span></div>`\n",
    "\n",
    "A bit of processing, and we can extract the desired text which will go a long way in identifying the transaction's class, in this case \"food\". The following function from `components/scripts/scraper.py` implements the above:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # type: ignore\n",
    "from bs4 import BeautifulSoup # type: ignore\n",
    "\n",
    "def google(query:str, verbose=True) -> str:\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    req = requests.get(\"https://www.google.com/search?q=\"+query, headers=headers)\n",
    "    req.raise_for_status()\n",
    "    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "    bus = soup.find_all(\"span\", class_=\"YhemCb\")  # potential results for business\n",
    "    comp = soup.find_all(\"div\", class_=\"wwUB2c PZPZlf\") # potential results for larger company\n",
    "    if len(bus) > 0:\n",
    "        bus = str(bus[-1])\n",
    "        bus = bus.split(\">\")[1].split(\"<\")[0]\n",
    "        if \" in \" in bus:\n",
    "            bus = bus.split(\" in \")[0]\n",
    "        print(\"✅ query: \" + query, \"result: \" + bus) if verbose else None\n",
    "        return bus\n",
    "    elif len(comp) == 1:\n",
    "            comp = str(comp[0])\n",
    "            comp = comp.split(\"</span>\")[0].split(\">\")[-1]\n",
    "            print(\"✅ query: \" + query, \"result (company): \" + comp) if verbose else None\n",
    "            return comp\n",
    "    else:\n",
    "        print(\"❌ couldn't find: \" + query) if verbose else None\n",
    "        return \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Categorising Results\n",
    "Once we have retrieved all google query results for each description in the test data, we have the job of categorising them into classes. To do this, I identify a few key indicators for each class (transfer and wages have been left out because it is unlikely that a google search would retrieve any information for these classes). For example, if the google result contains \"cafe\", \"bakery\" etc, then it's quite clearly food. Below is the method `categorise` which attempts to sort these results into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise(goog:str, verbose=True) -> Union[str, None]:\n",
    "    shopping = [\"market\", \"store\", \"shop\"]\n",
    "    food = [\"restaurant\", \"cafe\", \"bakery\", \"takeaway\", \"food\", \"coffee\", \"chicken\"]\n",
    "    beers = [\"bar\", \"pub\", \"hotel\"]\n",
    "    health = [\"dentist\", \"physiotherapist\", \"drug\", \"pharmacy\", \"pharmacist\"]\n",
    "    transport = [\"traffic\", \"parking\", \"carpark\"] # + [\"petrol\", \"gas\"]\n",
    "\n",
    "    if goog == \"\":\n",
    "        return np.NaN\n",
    "    \n",
    "    goog = goog.lower()\n",
    "\n",
    "    for feat in health:\n",
    "        if feat in goog:\n",
    "            print(\"✅ categorised \" + goog + \" as: \" + \"life/wellbeing\") if verbose else None\n",
    "            return \"life/wellbeing\"\n",
    "    for feat in food:\n",
    "        if feat in goog:\n",
    "            print(\"✅ categorised \" + goog + \" as: \" + \"food\") if verbose else None\n",
    "            return \"food\"\n",
    "    for feat in beers:\n",
    "        if feat in goog:\n",
    "            print(\"✅ categorised \" + goog + \" as: \" + \"beers\") if verbose else None\n",
    "            return \"beers\"\n",
    "    for feat in transport:\n",
    "        if feat in goog:\n",
    "            print(\"✅ categorised \" + goog + \" as: \" + \"transport\") if verbose else None\n",
    "            return \"transport\"\n",
    "    for feat in shopping:\n",
    "        if feat in goog:\n",
    "            print(\"✅ categorised \" + goog + \" as: \" + \"shopping\") if verbose else None\n",
    "            return \"shopping\"\n",
    "    print(\"❌ \" + \"couldn't categorise: \" + goog) if verbose else None\n",
    "    return np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Optimisation and Implementation\n",
    "Because Google only permits a certain number of google queries from a given IP adress per day, I needed to store my search results so that I didn't have to re-scrape them every time I ran my model (I only really ever had one shot per day to run my model, the second time Google would lock me out).\n",
    "\n",
    "To do this, I simply stored the search results for each transaction description in `components/files/google.csv`. Then, before I retrieved the google results from the test data the webscraping way, I would check to see if I had googled it previously by joining the training data to the data in the `google.csv` file. Then I would only ever scrape transactions that I hadn't googled previously before appending their results to the same `google.csv` to be used in the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_webscrape(testing):\n",
    "    # Join Webscraping\n",
    "    results = pd.read_csv(path + \"google.csv\")\n",
    "\n",
    "    googled = testing.merge(results, how=\"left\", on=\"desc_features\", validate=\"many_to_one\")\n",
    "    ungoogled = googled[pd.isna(googled.google)]\n",
    "    if len(ungoogled) > 0:\n",
    "        # Scrape any descriptions that haven't been googled before.\n",
    "        ungoogled = ungoogled[[\"desc_features\"]].drop_duplicates()\n",
    "        ungoogled[\"google\"] = [google(query) for query in ungoogled.desc_features]\n",
    "\n",
    "        # Record their results in the google.csv file\n",
    "        ungoogled.to_csv(path + \"google.csv\", mode = 'a', header = False, index=False)\n",
    "\n",
    "        # Join webscraping as before\n",
    "        results = pd.read_csv(path + \"google.csv\")\n",
    "        googled = tr_data.merge(results, how=\"left\", on=\"desc_features\", validate=\"many_to_one\")\n",
    "\n",
    "        # Assert that there is nothing left ungoogled\n",
    "        assert len(googled[pd.isna(googled.google)]) == 0\n",
    "\n",
    "    # Categorise \n",
    "    google_preds = [categorise(goog, verbose=False) for goog in googled.google]\n",
    "    return google_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 6. Evaluating the Model\n",
    "\n",
    "## 6.1 Implementing the Validation Function\n",
    "The following function evaluates the accuracy of a single fold, having both `web_scrape` and `lookup` as toggleable options, each of which make use of their respective methods defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(X:pd.DataFrame, split:Tuple[np.array, np.array], model:Any, web_scrape=False, lookup=False, verbose=True, min_freq=0) -> float:\n",
    "    train_index, test_index = split\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = X_train.category, X_test.category\n",
    "\n",
    "    # Web Scraping:\n",
    "    if web_scrape:\n",
    "        web_predictions = get_webscrape(X_test)\n",
    "    \n",
    "    if lookup:\n",
    "        lookup_predictions = get_lookup(X_train, X_test)\n",
    "        \n",
    "    corpus = get_corpora(X_train, min_freq=min_freq)\n",
    "\n",
    "    X_train = NLP_distances(X_train.drop('category', axis=1), corpus)\n",
    "    X_test = NLP_distances(X_test.drop('category', axis=1), corpus)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scale(X_train, scaler)\n",
    "    X_test = scale(X_test, scaler)\n",
    "    \n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    model_predictions = model.predict(X_test)\n",
    "    predictions = model_predictions\n",
    "    \n",
    "    if web_scrape or lookup:\n",
    "        print(\"model: \", predictions, type(predictions)) if verbose else None\n",
    "        if web_scrape:\n",
    "            print(\"web: \", web_predictions, type(web_predictions)) if verbose else None\n",
    "            predictions = np.array([web if str(web) != \"nan\" else mod for web, mod in zip(web_predictions, predictions)])\n",
    "        if lookup:\n",
    "            print(\"lookup: \", lookup_predictions, type(lookup_predictions)) if verbose else None\n",
    "            predictions = np.array([look if str(look) != 'nan' else pred for look, pred in zip(lookup_predictions, predictions)])\n",
    "        corrections = np.array([1 if pred!=mod else 0 for pred, mod in zip(predictions, model_predictions)])\n",
    "        baseline = np.array([1 if str(look) != 'nan' else 0 for look in lookup_predictions])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"predictions: \", predictions)\n",
    "        print(\"actual: \", np.array(y_test))\n",
    "        print(\"matches: \", predictions == np.array(y_test))\n",
    "\n",
    "    acc = sum(predictions == y_test)/len(predictions)\n",
    "\n",
    "    if verbose and (web_scrape or lookup):\n",
    "        print(f\"corrections: {sum(corrections)} of {len(corrections)}\")\n",
    "        print(f\"Baseline (just lookup without ML model): {sum(baseline)/len(baseline)}\")\n",
    "    \n",
    "    print(acc) if verbose else None\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Performing k-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 10-fold cross validation\n",
      "1 of 10\n",
      "2 of 10\n",
      "3 of 10\n",
      "4 of 10\n",
      "5 of 10\n",
      "6 of 10\n",
      "7 of 10\n",
      "8 of 10\n",
      "9 of 10\n",
      "10 of 10\n",
      "Average over 10 folds: 0.87\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Remove unknown transactions\n",
    "X = tr_data[~pd.isnull(tr_data.category)]\n",
    "\n",
    "splitter = StratifiedKFold(n_splits=n_folds, shuffle=True)\n",
    "folds = list(splitter.split(X.drop(\"category\", axis=1), X.category))\n",
    "\n",
    "print(f\"Performing {n_folds}-fold cross validation\")\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "upto = 1\n",
    "for fold in folds:\n",
    "    print(str(upto) + \" of \" + str(n_folds))\n",
    "    accuracies.append(run_fold(X, fold, model, web_scrape=True, lookup=True, verbose=False))\n",
    "    upto += 1\n",
    "\n",
    "acc = round(np.mean(accuracies),4)\n",
    "print(f\"Average over {n_folds} folds: {acc}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Using Multiprocessing for Cross-Validation\n",
    "We have written the `run_fold` function in this way so that we can easily implement multiprocessing to speed up the evaluation process. The following code achieves a 10-fold cross validation with multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average over 10 folds: 0.8717\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "def record(acc):\n",
    "    accuracies.append(acc)\n",
    "\n",
    "pool = mp.Pool(min(mp.cpu_count(), n_folds))\n",
    "for fold in folds:\n",
    "    pool.apply_async(run_fold, args=(X, fold, model, True, True, False), callback = record)\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "acc = round(np.mean(accuracies),4)\n",
    "print(f\"Average over {n_folds} folds: {acc}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
